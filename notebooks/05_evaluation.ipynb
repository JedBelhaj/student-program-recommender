{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7c00be",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2876af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61294c0",
   "metadata": {},
   "source": [
    "## Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15545e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "users = pd.read_csv(\"../data/raw/users.csv\")\n",
    "programs = pd.read_csv(\"../data/raw/programs.csv\")\n",
    "interactions = pd.read_csv(\"../data/raw/interactions.csv\")\n",
    "\n",
    "# Load models\n",
    "tfidf_vectorizer = joblib.load(\"../models/tfidf.pkl\")\n",
    "tfidf_matrix = joblib.load(\"../models/program_tfidf.pkl\")\n",
    "cf_model = joblib.load(\"../models/cf_svd.pkl\")\n",
    "\n",
    "# Extract CF components\n",
    "predicted_scores = cf_model[\"predicted_scores\"]\n",
    "user_id_map = cf_model[\"user_id_map\"]\n",
    "reverse_item_map = cf_model[\"reverse_item_map\"]\n",
    "interaction_matrix = cf_model[\"interaction_matrix\"]\n",
    "\n",
    "print(f\"Data loaded: {len(users)} users, {len(programs)} programs, {len(interactions)} interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433fcb62",
   "metadata": {},
   "source": [
    "## Create Ground Truth\n",
    "\n",
    "For evaluation, we need to know which programs each user actually liked (interacted with)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac193afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ground truth: programs each user interacted with\n",
    "ground_truth = defaultdict(set)\n",
    "for _, row in interactions.iterrows():\n",
    "    if row[\"interaction\"] == 1:  # Only positive interactions\n",
    "        ground_truth[row[\"user_id\"]].add(row[\"program_id\"])\n",
    "\n",
    "print(f\"Ground truth created for {len(ground_truth)} users\")\n",
    "print(f\"Average interactions per user: {np.mean([len(v) for v in ground_truth.values()]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff4c177",
   "metadata": {},
   "source": [
    "## Recommendation Functions\n",
    "\n",
    "Define the three recommendation approaches for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbdfe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_content_based(user_interests, k=5):\n",
    "    \"\"\"Content-based recommendations\"\"\"\n",
    "    user_vector = tfidf_vectorizer.transform([user_interests])\n",
    "    similarities = cosine_similarity(user_vector, tfidf_matrix).flatten()\n",
    "    top_indices = np.argsort(similarities)[::-1][:k]\n",
    "    return [programs.iloc[i][\"program_id\"] for i in top_indices]\n",
    "\n",
    "def recommend_cf(user_id, k=5):\n",
    "    \"\"\"Collaborative filtering recommendations\"\"\"\n",
    "    if user_id not in user_id_map:\n",
    "        return []  # Cold-start\n",
    "    \n",
    "    user_idx = user_id_map[user_id]\n",
    "    scores = predicted_scores[user_idx]\n",
    "    \n",
    "    # Filter out already-interacted programs\n",
    "    interacted_items = interaction_matrix[user_idx].nonzero()[1]\n",
    "    scores_copy = scores.copy()\n",
    "    scores_copy[interacted_items] = -np.inf\n",
    "    \n",
    "    top_items = np.argsort(scores_copy)[::-1][:k]\n",
    "    return [reverse_item_map[i] for i in top_items]\n",
    "\n",
    "def recommend_hybrid(user_id, user_interests, k=5, content_weight=0.6, cf_weight=0.4):\n",
    "    \"\"\"Hybrid recommendations\"\"\"\n",
    "    # Get content-based scores\n",
    "    user_vector = tfidf_vectorizer.transform([user_interests])\n",
    "    content_scores = cosine_similarity(user_vector, tfidf_matrix).flatten()\n",
    "    content_scores = content_scores / (content_scores.max() if content_scores.max() > 0 else 1)\n",
    "    \n",
    "    # Get CF scores (if available)\n",
    "    combined_scores = {}\n",
    "    \n",
    "    if user_id in user_id_map:\n",
    "        user_idx = user_id_map[user_id]\n",
    "        cf_scores = predicted_scores[user_idx]\n",
    "        \n",
    "        # Normalize CF scores\n",
    "        valid_scores = cf_scores[cf_scores > -np.inf]\n",
    "        if len(valid_scores) > 0:\n",
    "            min_score, max_score = valid_scores.min(), valid_scores.max()\n",
    "            score_range = max_score - min_score if max_score > min_score else 1\n",
    "            cf_scores_norm = (cf_scores - min_score) / score_range\n",
    "        else:\n",
    "            cf_scores_norm = cf_scores\n",
    "        \n",
    "        # Combine scores\n",
    "        for i, program_id in enumerate(programs[\"program_id\"]):\n",
    "            if program_id in reverse_item_map.values():\n",
    "                item_idx = [k for k, v in reverse_item_map.items() if v == program_id][0]\n",
    "                combined_scores[program_id] = content_weight * content_scores[i] + cf_weight * cf_scores_norm[item_idx]\n",
    "            else:\n",
    "                combined_scores[program_id] = content_weight * content_scores[i]\n",
    "    else:\n",
    "        # Cold-start: use only content-based\n",
    "        for i, program_id in enumerate(programs[\"program_id\"]):\n",
    "            combined_scores[program_id] = content_scores[i]\n",
    "    \n",
    "    # Sort and return top-k\n",
    "    sorted_programs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    return [p[0] for p in sorted_programs]\n",
    "\n",
    "print(\"âœ“ Recommendation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f35c27",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended, relevant, k):\n",
    "    \"\"\"Precision@k: fraction of recommended items that are relevant\"\"\"\n",
    "    recommended_k = recommended[:k]\n",
    "    relevant_count = len(set(recommended_k) & relevant)\n",
    "    return relevant_count / k if k > 0 else 0\n",
    "\n",
    "def recall_at_k(recommended, relevant, k):\n",
    "    \"\"\"Recall@k: fraction of relevant items that are recommended\"\"\"\n",
    "    recommended_k = recommended[:k]\n",
    "    relevant_count = len(set(recommended_k) & relevant)\n",
    "    return relevant_count / len(relevant) if len(relevant) > 0 else 0\n",
    "\n",
    "def ndcg_at_k(recommended, relevant, k):\n",
    "    \"\"\"NDCG@k: Normalized Discounted Cumulative Gain\"\"\"\n",
    "    recommended_k = recommended[:k]\n",
    "    \n",
    "    # DCG: sum of (relevance / log2(position + 1))\n",
    "    dcg = 0\n",
    "    for i, item in enumerate(recommended_k):\n",
    "        if item in relevant:\n",
    "            dcg += 1 / np.log2(i + 2)  # +2 because positions start at 1\n",
    "    \n",
    "    # IDCG: ideal DCG (if all relevant items were at the top)\n",
    "    idcg = sum([1 / np.log2(i + 2) for i in range(min(len(relevant), k))])\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "print(\"âœ“ Metrics implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37433b3",
   "metadata": {},
   "source": [
    "## Evaluate All Three Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation parameters\n",
    "K_VALUES = [1, 3, 5]\n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    \"content_based\": {k: {\"precision\": [], \"recall\": [], \"ndcg\": []} for k in K_VALUES},\n",
    "    \"collaborative\": {k: {\"precision\": [], \"recall\": [], \"ndcg\": []} for k in K_VALUES},\n",
    "    \"hybrid\": {k: {\"precision\": [], \"recall\": [], \"ndcg\": []} for k in K_VALUES}\n",
    "}\n",
    "\n",
    "# Track coverage (which programs get recommended)\n",
    "coverage = {\n",
    "    \"content_based\": set(),\n",
    "    \"collaborative\": set(),\n",
    "    \"hybrid\": set()\n",
    "}\n",
    "\n",
    "# Evaluate for each user\n",
    "test_users = users.head(50)  # Test on first 50 users for speed\n",
    "\n",
    "for _, user in test_users.iterrows():\n",
    "    user_id = user[\"user_id\"]\n",
    "    user_interests = user[\"interests_text\"]\n",
    "    \n",
    "    # Get ground truth for this user\n",
    "    relevant = ground_truth.get(user_id, set())\n",
    "    \n",
    "    if len(relevant) == 0:\n",
    "        continue  # Skip users with no interactions\n",
    "    \n",
    "    # Get recommendations from all three approaches\n",
    "    content_recs = recommend_content_based(user_interests, k=max(K_VALUES))\n",
    "    cf_recs = recommend_cf(user_id, k=max(K_VALUES))\n",
    "    hybrid_recs = recommend_hybrid(user_id, user_interests, k=max(K_VALUES))\n",
    "    \n",
    "    # Update coverage\n",
    "    coverage[\"content_based\"].update(content_recs)\n",
    "    coverage[\"collaborative\"].update(cf_recs)\n",
    "    coverage[\"hybrid\"].update(hybrid_recs)\n",
    "    \n",
    "    # Calculate metrics for each k\n",
    "    for k in K_VALUES:\n",
    "        # Content-based\n",
    "        results[\"content_based\"][k][\"precision\"].append(precision_at_k(content_recs, relevant, k))\n",
    "        results[\"content_based\"][k][\"recall\"].append(recall_at_k(content_recs, relevant, k))\n",
    "        results[\"content_based\"][k][\"ndcg\"].append(ndcg_at_k(content_recs, relevant, k))\n",
    "        \n",
    "        # Collaborative filtering\n",
    "        if len(cf_recs) > 0:\n",
    "            results[\"collaborative\"][k][\"precision\"].append(precision_at_k(cf_recs, relevant, k))\n",
    "            results[\"collaborative\"][k][\"recall\"].append(recall_at_k(cf_recs, relevant, k))\n",
    "            results[\"collaborative\"][k][\"ndcg\"].append(ndcg_at_k(cf_recs, relevant, k))\n",
    "        \n",
    "        # Hybrid\n",
    "        results[\"hybrid\"][k][\"precision\"].append(precision_at_k(hybrid_recs, relevant, k))\n",
    "        results[\"hybrid\"][k][\"recall\"].append(recall_at_k(hybrid_recs, relevant, k))\n",
    "        results[\"hybrid\"][k][\"ndcg\"].append(ndcg_at_k(hybrid_recs, relevant, k))\n",
    "\n",
    "print(f\"âœ“ Evaluation completed for {len(test_users)} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696bb87",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ad48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average metrics\n",
    "summary = []\n",
    "\n",
    "for approach in [\"content_based\", \"collaborative\", \"hybrid\"]:\n",
    "    for k in K_VALUES:\n",
    "        avg_precision = np.mean(results[approach][k][\"precision\"])\n",
    "        avg_recall = np.mean(results[approach][k][\"recall\"])\n",
    "        avg_ndcg = np.mean(results[approach][k][\"ndcg\"])\n",
    "        \n",
    "        summary.append({\n",
    "            \"Approach\": approach.replace(\"_\", \" \").title(),\n",
    "            \"k\": k,\n",
    "            \"Precision@k\": f\"{avg_precision:.3f}\",\n",
    "            \"Recall@k\": f\"{avg_recall:.3f}\",\n",
    "            \"NDCG@k\": f\"{avg_ndcg:.3f}\"\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(summary)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION SYSTEM EVALUATION RESULTS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb220c23",
   "metadata": {},
   "source": [
    "## Coverage Analysis\n",
    "\n",
    "Coverage measures what percentage of programs get recommended across all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_programs = len(programs)\n",
    "\n",
    "print(\"\\nCOVERAGE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "for approach, recommended_programs in coverage.items():\n",
    "    coverage_pct = len(recommended_programs) / total_programs * 100\n",
    "    print(f\"{approach.replace('_', ' ').title():20s}: {len(recommended_programs)}/{total_programs} programs ({coverage_pct:.1f}%)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f211b5",
   "metadata": {},
   "source": [
    "## Visualization: Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "metrics = [\"Precision@k\", \"Recall@k\", \"NDCG@k\"]\n",
    "approaches = [\"Content Based\", \"Collaborative\", \"Hybrid\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle(\"Recommendation System Performance Comparison\", fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, metric in enumerate([\"precision\", \"recall\", \"ndcg\"]):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for approach_key, approach_name in zip([\"content_based\", \"collaborative\", \"hybrid\"], approaches):\n",
    "        values = [np.mean(results[approach_key][k][metric]) for k in K_VALUES]\n",
    "        ax.plot(K_VALUES, values, marker='o', label=approach_name, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('k (Number of Recommendations)', fontsize=10)\n",
    "    ax.set_ylabel(metrics[idx], fontsize=10)\n",
    "    ax.set_title(metrics[idx], fontsize=11, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks(K_VALUES)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../models/evaluation_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Visualization saved to ../models/evaluation_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140ab6d",
   "metadata": {},
   "source": [
    "## Bar Chart: Approach Comparison at k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "metric_names = ['Precision', 'Recall', 'NDCG']\n",
    "metric_keys = ['precision', 'recall', 'ndcg']\n",
    "\n",
    "# Prepare data\n",
    "content_values = [np.mean(results[\"content_based\"][k][m]) for m in metric_keys]\n",
    "cf_values = [np.mean(results[\"collaborative\"][k][m]) for m in metric_keys]\n",
    "hybrid_values = [np.mean(results[\"hybrid\"][k][m]) for m in metric_keys]\n",
    "\n",
    "x = np.arange(len(metric_names))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width, content_values, width, label='Content-Based', color='#3498db')\n",
    "bars2 = ax.bar(x, cf_values, width, label='Collaborative', color='#e74c3c')\n",
    "bars3 = ax.bar(x + width, hybrid_values, width, label='Hybrid', color='#2ecc71')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title(f'Recommendation System Performance Comparison (k={k})', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{m}@{k}\" for m in metric_names])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../models/evaluation_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Comparison chart saved to ../models/evaluation_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9ceef",
   "metadata": {},
   "source": [
    "## Key Findings and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba343d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best approach for each metric at k=3\n",
    "k = 3\n",
    "best_approaches = {}\n",
    "\n",
    "for metric in [\"precision\", \"recall\", \"ndcg\"]:\n",
    "    scores = {\n",
    "        \"Content-Based\": np.mean(results[\"content_based\"][k][metric]),\n",
    "        \"Collaborative\": np.mean(results[\"collaborative\"][k][metric]),\n",
    "        \"Hybrid\": np.mean(results[\"hybrid\"][k][metric])\n",
    "    }\n",
    "    best_approaches[metric] = max(scores, key=scores.get)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ“Š Best Approach by Metric (k={k}):\")\n",
    "print(f\"   â€¢ Precision@{k}: {best_approaches['precision']}\")\n",
    "print(f\"   â€¢ Recall@{k}: {best_approaches['recall']}\")\n",
    "print(f\"   â€¢ NDCG@{k}: {best_approaches['ndcg']}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Coverage:\")\n",
    "for approach in [\"content_based\", \"collaborative\", \"hybrid\"]:\n",
    "    pct = len(coverage[approach]) / total_programs * 100\n",
    "    print(f\"   â€¢ {approach.replace('_', ' ').title()}: {pct:.1f}% of programs recommended\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(\"   â€¢ Higher precision = More accurate recommendations\")\n",
    "print(\"   â€¢ Higher recall = Better at finding all relevant programs\")\n",
    "print(\"   â€¢ Higher NDCG = Better ranking quality (relevant items at top)\")\n",
    "print(\"   â€¢ Higher coverage = More diversity in recommendations\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a98458",
   "metadata": {},
   "source": [
    "## Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbda358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv(\"../models/evaluation_results.csv\", index=False)\n",
    "print(\"âœ“ Results saved to ../models/evaluation_results.csv\")\n",
    "\n",
    "# Save detailed metrics\n",
    "evaluation_summary = {\n",
    "    \"results\": results,\n",
    "    \"coverage\": {k: list(v) for k, v in coverage.items()},\n",
    "    \"best_approaches\": best_approaches,\n",
    "    \"k_values\": K_VALUES\n",
    "}\n",
    "\n",
    "joblib.dump(evaluation_summary, \"../models/evaluation_summary.pkl\")\n",
    "print(\"âœ“ Detailed evaluation saved to ../models/evaluation_summary.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85676e4e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Evaluation Complete! âœ…**\n",
    "\n",
    "**Metrics Evaluated:**\n",
    "- âœ… Precision@k (1, 3, 5)\n",
    "- âœ… Recall@k (1, 3, 5)\n",
    "- âœ… NDCG@k (1, 3, 5)\n",
    "- âœ… Coverage analysis\n",
    "\n",
    "**Approaches Compared:**\n",
    "- âœ… Content-Based Filtering\n",
    "- âœ… Collaborative Filtering (SVD)\n",
    "- âœ… Hybrid (60% content + 40% CF)\n",
    "\n",
    "**Outputs Generated:**\n",
    "- Results table with all metrics\n",
    "- Line charts showing metric trends\n",
    "- Bar chart comparison at k=3\n",
    "- Coverage analysis\n",
    "- CSV export for report\n",
    "\n",
    "**Next Steps:**\n",
    "1. Build FastAPI application\n",
    "2. Create UI for user interaction\n",
    "3. Add feedback logging\n",
    "4. Write final report with these results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
