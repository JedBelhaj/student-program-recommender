{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447b3bf3",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with SVD\n",
    "\n",
    "This notebook implements a **collaborative filtering (CF)** approach using TruncatedSVD to leverage historical interaction data.\n",
    "\n",
    "## Why Collaborative Filtering?\n",
    "- Discovers hidden patterns from user-program interactions\n",
    "- Can recommend programs based on what similar users liked\n",
    "- Complements content-based filtering for a hybrid system\n",
    "- Effective for personalization as interaction data grows\n",
    "\n",
    "## Why TruncatedSVD?\n",
    "- **Matrix factorization**: Decomposes user-item matrix into latent factors\n",
    "- **Efficient**: Works well with sparse matrices (most users haven't interacted with most programs)\n",
    "- **Interpretable**: Latent dimensions capture hidden preference patterns\n",
    "- **scikit-learn**: No compilation required, works on all Python versions\n",
    "- Fast and scalable for production use\n",
    "\n",
    "## Import Libraries\n",
    "TruncatedSVD from scikit-learn provides efficient matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea5e1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf663ef",
   "metadata": {},
   "source": [
    "## Load Data and Create Train/Test Split\n",
    "\n",
    "Load all data and split interactions 80/20 for training/testing.\n",
    "\n",
    "**Critical:** CF model must be trained on training data only (80%) to avoid data leakage during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dc0183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 500 users, 50 programs, 8004 interactions\n",
      "\n",
      "Train/Test Split:\n",
      "  Training: 6403 interactions (80.0%)\n",
      "  Testing:  1601 interactions (20.0%)\n",
      "Unique users in training: 500\n",
      "Unique programs in training: 50\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "users = pd.read_csv(\"../data/raw/users.csv\")\n",
    "programs = pd.read_csv(\"../data/raw/programs.csv\")\n",
    "interactions = pd.read_csv(\"../data/raw/interactions.csv\")\n",
    "\n",
    "print(f\"Total data: {len(users)} users, {len(programs)} programs, {len(interactions)} interactions\")\n",
    "\n",
    "# Split interactions 80/20 for train/test\n",
    "train_interactions, test_interactions = train_test_split(\n",
    "    interactions, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Save for later use\n",
    "train_interactions.to_csv(\"../data/processed/train_interactions.csv\", index=False)\n",
    "test_interactions.to_csv(\"../data/processed/test_interactions.csv\", index=False)\n",
    "\n",
    "print(f\"\\nTrain/Test Split:\")\n",
    "print(f\"  Training: {len(train_interactions)} interactions ({len(train_interactions)/len(interactions)*100:.1f}%)\")\n",
    "print(f\"  Testing:  {len(test_interactions)} interactions ({len(test_interactions)/len(interactions)*100:.1f}%)\")\n",
    "\n",
    "# Create user and item ID mappings from training data\n",
    "user_ids = train_interactions[\"user_id\"].unique()\n",
    "program_ids = train_interactions[\"program_id\"].unique()\n",
    "\n",
    "user_id_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "item_id_map = {pid: idx for idx, pid in enumerate(program_ids)}\n",
    "\n",
    "reverse_item_map = {idx: pid for pid, idx in item_id_map.items()}\n",
    "\n",
    "print(f\"Unique users in training: {len(user_ids)}\")\n",
    "print(f\"Unique programs in training: {len(program_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82695f3e",
   "metadata": {},
   "source": [
    "## Create ID Mappings\n",
    "Map user and program IDs to matrix indices for building the interaction matrix.\n",
    "\n",
    "This establishes the vocabulary for both users and items before building the sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88a2ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction matrix shape: (500, 50)\n",
      "Number of training interactions: 6403\n"
     ]
    }
   ],
   "source": [
    "# Build sparse user-item interaction matrix from TRAINING data only\n",
    "row_indices = [user_id_map[uid] for uid in train_interactions[\"user_id\"]]\n",
    "col_indices = [item_id_map[pid] for pid in train_interactions[\"program_id\"]]\n",
    "data = train_interactions[\"interaction\"].values\n",
    "\n",
    "interaction_matrix = csr_matrix(\n",
    "    (data, (row_indices, col_indices)),\n",
    "    shape=(len(user_ids), len(program_ids))\n",
    ")\n",
    "\n",
    "print(f\"Interaction matrix shape: {interaction_matrix.shape}\")\n",
    "print(f\"Number of training interactions: {interaction_matrix.nnz}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519cbede",
   "metadata": {},
   "source": [
    "## Build Interaction Matrix\n",
    "Convert **training interactions only** into a sparse user-item matrix format.\n",
    "\n",
    "**Important:** Using only 80% training data to build the model. The remaining 20% test data will be used for evaluation in notebook 05.\n",
    "\n",
    "**Format:** Rows = users, Columns = programs, Values = interaction (1 = liked/viewed)\n",
    "\n",
    "- Using CSR (Compressed Sparse Row) format for efficiency- Most entries are 0 (users haven't interacted with most programs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b2cc376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User factors shape: (500, 20)\n",
      "Item factors shape: (50, 20)\n",
      "Explained variance ratio: 0.546\n"
     ]
    }
   ],
   "source": [
    "# Train SVD model with more components for larger dataset\n",
    "n_components = min(20, len(program_ids) - 1)  # Adaptive based on program count\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "user_factors = svd.fit_transform(interaction_matrix)\n",
    "item_factors = svd.components_.T\n",
    "\n",
    "print(f\"User factors shape: {user_factors.shape}\")\n",
    "print(f\"Item factors shape: {item_factors.shape}\")\n",
    "print(f\"Explained variance ratio: {svd.explained_variance_ratio_.sum():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babcd5ef",
   "metadata": {},
   "source": [
    "## Train SVD Model\n",
    "\n",
    "**How it works:**\n",
    "- Decomposes the user-item matrix into two lower-dimensional matrices\n",
    "- **User factors**: Each user represented by latent features\n",
    "- **Item factors**: Each program represented by latent features\n",
    "- Predictions = dot product of user and item factors\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `n_components`: Latent factor dimension (adaptive based on dataset size)\n",
    "  - Lower = simpler patterns, less overfitting\n",
    "  - Higher = more complex patterns\n",
    "  - Set to min(20, num_programs - 1) for optimal performance\n",
    "\n",
    "**Explained variance**: Shows how much of the interaction patterns are captured by the latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2acd5a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted scores shape: (500, 50)\n",
      "Score range: [-0.620, 1.442]\n"
     ]
    }
   ],
   "source": [
    "# Compute predicted scores for all user-item pairs\n",
    "predicted_scores = user_factors @ item_factors.T\n",
    "\n",
    "\n",
    "print(f\"Predicted scores shape: {predicted_scores.shape}\")\n",
    "print(f\"Score range: [{predicted_scores.min():.3f}, {predicted_scores.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53152444",
   "metadata": {},
   "source": [
    "## Compute Predictions\n",
    "Calculate predicted interaction scores for all user-program pairs using matrix multiplication.\n",
    "\n",
    "**Result:** Each user gets a predicted score for every program, representing how likely they are to like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbcfc3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing recommendations for user: u_0067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'program_id': 'p_012', 'score': np.float64(0.6177207136133009)},\n",
       " {'program_id': 'p_025', 'score': np.float64(0.5879447565336278)},\n",
       " {'program_id': 'p_038', 'score': np.float64(0.44512790154136594)}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recommend_cf(user_id, k=3):\n",
    "    \"\"\"Generate top-k recommendations for a user using CF\"\"\"\n",
    "    if user_id not in user_id_map:\n",
    "        return []  # Handle cold-start users\n",
    "    \n",
    "    user_idx = user_id_map[user_id]\n",
    "    scores = predicted_scores[user_idx]\n",
    "    \n",
    "    # Filter out programs the user has already interacted with\n",
    "    interacted_items = interaction_matrix[user_idx].nonzero()[1]\n",
    "    scores_copy = scores.copy()\n",
    "    scores_copy[interacted_items] = -np.inf\n",
    "    \n",
    "    # Get top-k programs\n",
    "    top_items = np.argsort(scores_copy)[::-1][:k]\n",
    "    recommendations = [\n",
    "        {\"program_id\": reverse_item_map[i], \"score\": scores[i]}\n",
    "        for i in top_items\n",
    "    ]\n",
    "    return recommendations\n",
    "\n",
    "# Test with an actual user from the training data\n",
    "test_user = list(user_id_map.keys())[0]\n",
    "print(f\"Testing recommendations for user: {test_user}\")\n",
    "recommend_cf(test_user)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95682255",
   "metadata": {},
   "source": [
    "## Recommendation Function\n",
    "Generate top-k program recommendations for a given user based on CF predictions.\n",
    "\n",
    "**Process:**\n",
    "1. Convert user ID to internal index\n",
    "2. Get predicted scores for all programs\n",
    "3. **Filter out already-interacted programs** (don't recommend what they've seen)\n",
    "4. Sort by score (descending)\n",
    "5. Return top-k program IDs with scores\n",
    "\n",
    "**Key improvement:** Excludes programs the user has already liked/viewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68c20749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CF model saved to ../models/cf_svd.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save SVD model and mappings\n",
    "joblib.dump({\n",
    "    \"svd\": svd,\n",
    "    \"user_factors\": user_factors,\n",
    "    \"item_factors\": item_factors,\n",
    "    \"predicted_scores\": predicted_scores,\n",
    "    \"user_id_map\": user_id_map,\n",
    "    \"item_id_map\": item_id_map,\n",
    "    \"reverse_item_map\": reverse_item_map,\n",
    "    \"interaction_matrix\": interaction_matrix\n",
    "}, \"../models/cf_svd.pkl\")\n",
    "\n",
    "print(\"✓ CF model saved to ../models/cf_svd.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9785c2",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "Persist the trained SVD model, factors, and mappings for deployment.\n",
    "\n",
    "**Saved components:**\n",
    "- SVD model (for retraining if needed)\n",
    "\n",
    "- User and item latent factors- Interaction matrix (to filter already-seen items)\n",
    "\n",
    "- Precomputed predicted scores (fast inference)- ID mappings (external ↔ internal indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
